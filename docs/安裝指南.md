# 安裝指南

本指南將引導您安裝 `audio_transcriber` 及所有必要的相依套件，包括可選的 GPU 加速功能以提升轉錄速度。

## 目錄

1. [系統需求](#系統需求)
2. [建議：使用 Linux 或 WSL](#建議使用-linux-或-wsl)
3. [步驟一：安裝 NVIDIA CUDA 函式庫（GPU 支援）](#步驟一安裝-nvidia-cuda-函式庫gpu-支援)
4. [步驟二：安裝 audio_transcriber](#步驟二安裝-audio_transcriber)
5. [步驟三：安裝 Ollama](#步驟三安裝-ollama)
6. [驗證安裝](#驗證安裝)
7. [疑難排解](#疑難排解)

---

## 系統需求

- **Python**：3.11 以上
- **記憶體**：最低 8GB（建議 16GB）
- **儲存空間**：10GB 可用空間
- **GPU（選用）**：支援 CUDA 的 NVIDIA GPU

### 支援的作業系統

- **Linux**（Ubuntu 20.04+、Fedora、Arch）— 建議使用
- **WSL**（Windows Subsystem for Linux）— 建議 Windows 使用者使用
- Windows 10/11（原生支援，但設定較複雜）
- macOS（Intel 或 Apple Silicon）

### Python 安裝

確認已安裝 Python 3.11 以上版本：

```bash
python --version
```

若未安裝，請至 https://www.python.org/downloads/ 下載，或使用系統套件管理器：

```bash
# Ubuntu/Debian
sudo apt install python3.11 python3.11-venv

# macOS (Homebrew)
brew install python@3.11
```

---

## 建議：使用 Linux 或 WSL

**我們強烈建議使用 Linux 或 WSL 來安裝此套件。** Linux/WSL 提供更簡易的 CUDA 設定、與 faster-whisper 更好的相容性，以及更簡單的環境配置。

### 安裝 WSL（Windows 使用者）

官方指南：https://learn.microsoft.com/en-us/windows/wsl/install

快速安裝（Windows 10 version 2004 以上或 Windows 11）：

```powershell
wsl --install
```

重新啟動電腦並完成 Ubuntu 設定。

### WSL GPU 支援

WSL 2 自動支援 CUDA，只要您有：
- 在 Windows 上安裝最新的 NVIDIA GPU 驅動程式
- Windows 本身不需要安裝 CUDA

> **重要**：請勿在 WSL 內安裝 NVIDIA 驅動程式。Windows 驅動程式會透過 stub library 自動為 WSL 提供 CUDA 支援。您只需要在 WSL 內安裝 CUDA 函式庫（請參閱步驟一）。

詳細的 WSL GPU 設定，請參閱：https://learn.microsoft.com/en-us/windows/wsl/tutorials/gpu-compute

---

## 步驟一：安裝 NVIDIA CUDA 函式庫套件（GPU 支援）

GPU 加速可提供約 4-10 倍的速度提升。此步驟為**選用**——若沒有 GPU，`faster-whisper` 將使用 CPU 執行。

### 需求

`faster-whisper` 使用 CTranslate2，需要：
- **CUDA 12.x**（建議 12.3 以上）
- **cuDNN 9.x**
- **cuBLAS**（隨 CUDA 或 pip 套件提供）

### 方法一：透過 pip 安裝（Linux/WSL — 建議）

```bash
pip install nvidia-cublas-cu12 nvidia-cudnn-cu12==9.*
```

**重要**：使用 pip 安裝的 CUDA 函式庫時，必須設定 `LD_LIBRARY_PATH`。請將以下內容加入 `~/.bashrc`：

```bash
export LD_LIBRARY_PATH=$(python3 -c 'import os; import nvidia.cublas.lib; import nvidia.cudnn.lib; print(os.path.dirname(nvidia.cublas.lib.__file__) + ":" + os.path.dirname(nvidia.cudnn.lib.__file__))')
```

重新載入 shell：

```bash
source ~/.bashrc
```

### 方法二：安裝 CUDA Toolkit（Linux/WSL — 系統層級（system-wide）安裝）

系統層級安裝（不需要設定 `LD_LIBRARY_PATH`）：

```bash
# Ubuntu 22.04
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo apt update
sudo apt install cuda-toolkit-12-3
sudo apt install cudnn9-cuda-12
```

加入 `~/.bashrc`：

```bash
export PATH=/usr/local/cuda/bin:$PATH
export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
```

### 方法三：Windows 原生環境（不建議）

若必須使用 Windows 原生環境：

1. 安裝 CUDA Toolkit：https://developer.nvidia.com/cuda-downloads
2. 透過 pip 安裝 cuDNN（現代方法，無需手動複製 DLL）：

```powershell
pip install nvidia-cudnn-cu12==9.*
```

> **注意**：Windows 不需要手動設定 PATH——CUDA 安裝程式會自動處理。

### 方法四：Docker

```bash
docker pull nvidia/cuda:12.3.2-cudnn9-runtime-ubuntu22.04
```

---

## 步驟二：安裝 audio_transcriber

### 複製儲存庫

```bash
git clone <https://github.com/Solomon97/audio_transcriber>
cd audio_transcriber
```

### 建立虛擬環境（建議）

```bash
python -m venv venv
source venv/bin/activate    # Linux/macOS/WSL
# venv\Scripts\activate     # Windows 原生
```

### 安裝套件

**基本安裝**（僅轉錄功能）：

```bash
pip install -e .
```

**包含 Ollama 支援**（新增 LLM 功能的 Python 客戶端）：

```bash
pip install -e ".[ollama]"
```

> **注意**：這會安裝 `ollama` Python 客戶端函式庫，用於與 Ollama 伺服器通訊。您仍需要另外安裝 Ollama 應用程式（步驟三）。

**開發安裝**：

```bash
pip install -e ".[dev,ollama]"
```

---

## 步驟三：安裝 Ollama

Ollama 是後處理功能（清理、摘要、翻譯、擷取）所需的 LLM 伺服器。若您只需要轉錄功能，可跳過此步驟。

### Linux / WSL

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

驗證：

```bash
ollama --version
```

### macOS

```bash
brew install ollama
brew services start ollama
```

### Windows 原生

從以下網址下載並安裝：https://ollama.com/download

### 從 WSL 使用 Windows 上的 Ollama

若 Ollama 安裝在 Windows 上，而您想從 WSL 存取：

1. 建立/編輯 `%USERPROFILE%\.ollama\config.toml`：

```toml
OLLAMA_HOST = "0.0.0.0"
```

2. 在 Windows 上重新啟動 Ollama

3. 在 WSL Python 程式碼中：

```python
from audio_transcriber import PostProcessor
processor = PostProcessor(base_url="http://host.docker.internal:11434")
```

### 下載建議的模型

```bash
ollama pull yi:9b          # 中文（繁體）
ollama pull llama3.1:8b    # 英文
ollama pull qwen2.5:7b     # 日文、韓文、通用
```

驗證：

```bash
ollama list
```

---

## 驗證安裝

### 測試轉錄功能

```python
from audio_transcriber import Transcriber

transcriber = Transcriber(model_size="large-v3")
print("Transcriber 載入成功！")
print(f"裝置：{transcriber.device}")
```

### 測試 GPU 支援

```python
from faster_whisper import WhisperModel

try:
    model = WhisperModel("tiny", device="cuda", compute_type="float16")
    print("GPU 支援正常運作！")
except Exception as e:
    print(f"GPU 不可用：{e}")
```

### 測試 Ollama

```python
from audio_transcriber import PostProcessor

try:
    processor = PostProcessor()
    print("Ollama 連線成功！")
except Exception as e:
    print(f"Ollama 錯誤：{e}")
```

### 執行範例腳本

```bash
python examples/transcribe_auto_detect.py your_audio.wav
```

---

## 疑難排解

### LD_LIBRARY_PATH 錯誤（Linux/WSL）

若使用 pip 安裝的 CUDA 函式庫並出現找不到函式庫的錯誤：

1. 確認 export 指令已加入 `~/.bashrc`
2. 重新載入：`source ~/.bashrc`
3. 驗證：`echo $LD_LIBRARY_PATH`

> 這只適用於 pip 安裝的 CUDA（方法一）。系統層級安裝不需要此設定。

### 找不到 DLL 錯誤（Windows 原生環境）

CUDA 安裝程式應會自動設定 PATH。若仍出現錯誤：

1. 重新啟動終端機/PowerShell
2. 確認 CUDA 已安裝：檢查 `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.x\bin\` 是否存在
3. 若需要，透過「系統內容」→「環境變數」手動加入 PATH

### ctranslate2 版本不符

```bash
pip install --force-reinstall ctranslate2
```

若使用較舊的 CUDA 版本：

```bash
# CUDA 11 + cuDNN 8
pip install --force-reinstall ctranslate2==3.24.0

# CUDA 12 + cuDNN 8
pip install --force-reinstall ctranslate2==4.4.0
```

### Ollama 連線被拒絕

```bash
ollama serve
curl http://localhost:11434/api/tags
```

### Ollama 找不到模型

```bash
ollama pull yi:9b
ollama pull qwen2.5:7b
```

### CPU 轉錄速度較慢屬正常現象

使用較小的模型或 int8 精度：

```python
transcriber = Transcriber(model_size="medium", compute_type="int8")
```

### CUDA 記憶體不足

```python
# 使用較小的模型
transcriber = Transcriber(model_size="medium")

# 或使用 int8 精度
transcriber = Transcriber(model_size="large-v3", compute_type="int8_float16")
```

---

## 快速開始摘要

```bash
# 0.（Windows 使用者）先安裝 WSL
#    https://learn.microsoft.com/en-us/windows/wsl/install

# 1. 安裝 CUDA 函式庫（GPU 支援）
pip install nvidia-cublas-cu12 nvidia-cudnn-cu12==9.*
# 將 LD_LIBRARY_PATH 加入 ~/.bashrc（請參閱步驟一）

# 2. 安裝 audio_transcriber
git clone <repository-url>
cd audio_transcriber
pip install -e ".[ollama]"

# 3. 安裝 Ollama
curl -fsSL https://ollama.com/install.sh | sh
ollama pull yi:9b

# 4. 測試
python -c "from audio_transcriber import Transcriber; print('成功！')"
```

---

## 取得協助

1. [faster-whisper GitHub issues](https://github.com/SYSTRAN/faster-whisper/issues)
2. [Ollama 文件](https://ollama.com/docs)
3. 聯絡：solomonc@alum.mit.edu